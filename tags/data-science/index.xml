<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Science on Darren Dube</title><link>/tags/data-science/</link><description>Recent content in Data Science on Darren Dube</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 18 Jun 2022 00:00:00 +0200</lastBuildDate><atom:link href="/tags/data-science/index.xml" rel="self" type="application/rss+xml"/><item><title>Making decisions from dirty data can have disastrous outcomes</title><link>/blog/post-4/</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0200</pubDate><guid>/blog/post-4/</guid><description>&lt;p>In 2016, Microsoft released &lt;a href="https://en.wikipedia.org/wiki/Tay_%28bot%29"




 target="_blank"
 


>Tay&lt;/a>, an AI chatbot designed to interact with Twitter users, answering their questions, and learning from users&amp;rsquo; replies. Unlike the other chatbots of the time, however, Tay was meant to have a personality, and to have a sense of humour, engaging in “casual and playful conversation”.&lt;/p></description></item></channel></rss>